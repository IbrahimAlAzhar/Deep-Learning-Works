{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AA using word level CNN LSTM.ipynb","provenance":[],"authorship_tag":"ABX9TyP+TJpxN8hJ+wW7ArlrNtkO"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"nxfjX5bOlV2W","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600807968445,"user_tz":-360,"elapsed":1299,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import tensorflow\n","import keras\n","import nltk\n","import gensim\n","\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from keras.models import Sequential\n","from keras.utils import np_utils\n","from keras.preprocessing.sequence import pad_sequences\n","# from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout,SpatialDropout1D, Bidirectional,Flatten\n","from keras.models import Model\n","from keras.optimizers import Adam\n","from keras.layers.normalization import BatchNormalization\n","\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.pipeline import Pipeline\n","from sklearn import model_selection, preprocessing\n","from sklearn.model_selection import train_test_split\n","\n","from scipy.cluster.vq import whiten\n","\n","from collections import Counter\n","\n","from gensim.models import Word2Vec\n","from gensim.models import FastText\n","\n","from sklearn.metrics import f1_score"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"G2W6dhVkllTH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1600807975610,"user_tz":-360,"elapsed":1357,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"e74a8bfe-ae4a-4b51-dd99-518bd3fa9ccb"},"source":["import string\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn import model_selection, preprocessing\n","from sklearn.model_selection import train_test_split\n","import io\n","import matplotlib.pyplot as plt\n","\n","from __future__ import print_function\n","from __future__ import division\n","import json\n","import numpy as np\n","\n","from keras.models import Model\n","from keras.optimizers import SGD, Adam\n","from keras.layers import Input, Dense, Dropout, Flatten, Lambda, Embedding, LSTM,Bidirectional, concatenate, BatchNormalization \n","from keras.layers.convolutional import Convolution1D, MaxPooling1D\n","from keras.initializers import RandomNormal\n","import tensorflow as tf\n","from sklearn.metrics import classification_report,confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import tensorflow\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from keras.utils import np_utils\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.pipeline import Pipeline\n","\n","import pickle"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"O-4pINZZlleY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600808020489,"user_tz":-360,"elapsed":31714,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"f22ddb9e-c5f6-4b01-9bb7-aa83a29ad9fc"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B8LNiDFnllkP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600808033164,"user_tz":-360,"elapsed":1263,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}}},"source":["import os\n","from pathlib import Path\n","\n","path = Path(os.getcwd())\n","path = path/'gdrive'/'My Drive'\n","#news=path/'Thesis Data'/'full doc csv'\n","aa=path/'Thesis Data'/'Our dataset'\n","#aa2=path/'Thesis Data'/'AA dataset'\n","#char=path/'Thesis Data'/'char-ulm'\n","#wiki_data = path/'Thesis Data'/'wiki_data'/'Wiki'"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"DisSudP2llgU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600808051607,"user_tz":-360,"elapsed":1253,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}}},"source":["def load_ag_data(trainfile,testfile):\n","  \n","  onehotencoder = preprocessing.OneHotEncoder(categories='auto')\n","\n","  ## TRAIN\n","  df = pd.read_csv(trainfile)\n","  df = df.dropna()\n","  df = df.sample(frac=1).reset_index(drop=True)#shuffle\n","  x_train = df['text'].tolist()\n","  y_train = onehotencoder.fit_transform((df['label'].values).reshape(-1,1)).toarray()\n","\n","  ## TEST\n","  df = pd.read_csv(testfile)\n","  df = df.dropna()\n","  df = df.sample(frac=1).reset_index(drop=True)#shuffle\n","  x_test = df['text'].tolist()\n","  y_test = onehotencoder.transform((df['label'].values).reshape(-1,1)).toarray()\n","\n","  return (x_train, y_train), (x_test, y_test)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"b__7xGy-mF44","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600808067421,"user_tz":-360,"elapsed":2345,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}}},"source":["def create_vocab_set(data,MAX_SEQUENCE_LENGTH=750):\n","    vocab=Counter()\n","    for i,s in enumerate(data): # here use data's index and data\n","      vocab.update(s.split()) # split the 's'(data) and update on 'vocab',so vocab has all word in dataset\n","\n","    with open(we,\"rb\") as input_file: # open 'rb' as a input file\n","      model = pickle.load(input_file) # python pickle is used to serialize and deserialize a python object structure\n","    word_vectors = model.wv \n","    MAX_NB_WORDS = len(word_vectors.vocab) # 'MAX_NB_WORDS' is len of word_vectors vocabulary\n","    nb_words = min(MAX_NB_WORDS, len(word_vectors.vocab)) + 1\n","    word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NB_WORDS))}\n","    return vocab,word_index,nb_words,word_vectors,MAX_NB_WORDS"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"gV2b64VFmJeo","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600808082253,"user_tz":-360,"elapsed":1535,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}}},"source":["def encode_data(x, word_index, mxlen):\n","#   ret = [[word_index.get(t, 0) for t in sent.split() ] for sent in x]\n","  ret = []\n","  for sent in x: # loop over each sentence through dataset\n","    ls = [word_index.get(t, 0) for t in sent.split()] # each word in sentence take the 'word' and store it to 'ls',get method returns the value for the given key,if present in the dictionary.\n","    # padding\n","    if len(ls)>=mxlen: # if len(ls) is greater then 'mxlen' then we remove the extra area of 'ls' and update it\n","      ls = ls[:mxlen]\n","    else : # if len(ls) is smaller then mxlen then we apply zero padding of the extra area and add it to 'ls'\n","      ls=ls+[0]*(mxlen-len(ls))\n","    ret.append(ls) # after each sentence we append it to 'ret' list\n","  return np.array(ret) # return the 'ret' list as a np array"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"fd9AzJ72mNTv","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600808097689,"user_tz":-360,"elapsed":1282,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}}},"source":["def form_we_matrix(word_index,nb_words,word_vectors,MAX_NB_WORDS,WV_DIM=300):\n","  # we initialize the matrix with random numbers\n","  wv_matrix = (np.random.rand(nb_words, WV_DIM) - 0.5) / 5.0\n","\n","  for word, i in word_index.items():\n","      if i >= MAX_NB_WORDS:\n","          continue\n","      try:\n","          embedding_vector = word_vectors[word]\n","          wv_matrix[i] = embedding_vector\n","      except:\n","          pass\n","  return wv_matrix"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nh3Vjd5bmRJC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600808111076,"user_tz":-360,"elapsed":1671,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}}},"source":["def create_model(filter_kernels, dense_outputs, maxlen, vocab_size, nb_filter, cat_output, w, WV_DIM=300):\n","    \n","    initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)\n","\n","    # Define what the input shape looks like\n","    inputs = Input(shape=(maxlen,), dtype='int64')\n","\n","    # print(w)\n","    embedded = Embedding(input_dim=vocab_size, output_dim=WV_DIM , weights=[w])(inputs)\n","\n","    # All the convolutional layers...\n","    conv = Convolution1D(filters=nb_filter[0], kernel_size=filter_kernels[0], kernel_initializer=initializer,\n","                         padding='valid', activation='relu',\n","                         input_shape=(maxlen, vocab_size))(embedded)\n","    conv = MaxPooling1D(pool_size=3)(conv)\n","\n","    conv1 = Convolution1D(filters=nb_filter[1], kernel_size=filter_kernels[1], kernel_initializer=initializer,\n","                          padding='valid', activation='relu')(conv)\n","    conv1 = MaxPooling1D(pool_size=3)(conv1)\n","\n","    lstm=Dropout(0.5)(LSTM(100)(conv1)) # after conv1 we use 100 'lstm' with dropout rate 0.5 \n","    z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(lstm)) # applying 'relu' activation on top of 'lstm' layers\n","\n","    # Output dense layer with softmax activation\n","    pred = Dense(cat_output, activation='softmax', name='output')(z)\n","\n","    model = Model(inputs=inputs, outputs=pred)\n","\n","    adam = Adam(lr=0.001, decay=0.0001)\n","    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n","\n","    return model\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"dmdr2atGmUT1","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600808125862,"user_tz":-360,"elapsed":1248,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}}},"source":["def CNN_LSTM(trainfile,testfile,aa,cat_output):\n","  np.random.seed(123)  # for reproducibility\n","  # Maximum length. Longer gets chopped. Shorter gets padded.\n","  maxlen = 750\n","  # Model params\n","  # Filters for conv layers\n","  nb_filter = [128,256]\n","  # Number of units in the dense layer\n","  dense_outputs = 512\n","  # Conv layer kernel size\n","  filter_kernels = [7, 3]\n","  # Compile/fit params\n","  batch_size = 128\n","  nb_epoch = 15\n","\n","  print('Loading data...')\n","  (xt, yt), (x_test, y_test) = load_ag_data(trainfile,testfile)\n","\n","  print('Creating vocab...')\n","  vocab,word_index,nb_words,word_vectors,MAX_NB_WORDS = create_vocab_set(xt)\n","\n","  print('Form Word Embedding Matrix...')\n","  w = form_we_matrix(word_index,nb_words,word_vectors,MAX_NB_WORDS)\n","  print(\"word embedding shape = \",w.shape)\n","  \n","  print('Build model...')\n","  model = create_model(filter_kernels, dense_outputs, maxlen, nb_words, nb_filter, cat_output,w)\n","  \n","  print('Encode data...')\n","  xt = encode_data(xt, word_index, maxlen)\n","  x_test = encode_data(x_test, word_index, maxlen)\n","\n","  model.summary()\n","  print('Fit model...')\n","  hist = model.fit(xt, yt, validation_split=0.2 , batch_size=batch_size, epochs=nb_epoch, shuffle=True)\n","\n","  history = pd.DataFrame(hist.history)\n","  plt.figure(figsize=(12,12));\n","  plt.plot(history[\"loss\"],label='loss');\n","  plt.plot(history[\"val_loss\"], label='val-loss');\n","  plt.title(\"Loss with trained word vectors\");\n","  plt.legend();\n","  # plt.show();\n","  plt.savefig(aa+'loss.eps')\n","  \n","  Y_pred = model.predict(x_test)\n","  y_pred = np.argmax(Y_pred, axis=1)\n","  y_test = np.argmax(y_test,axis=1)\n","  # print(y_pred)\n","  # print(y_test)\n","  acc = (y_test==y_pred).sum()/len(y_pred)\n","  f1 = f1_score(y_test, y_pred, average='macro')\n","  print(\"Accuracy=\",acc,\" F1=\",f1)\n","  cm=confusion_matrix(y_test, y_pred)\n","  print(cm)\n","\n","  df_cm = pd.DataFrame(cm, range(cat_output),range(cat_output))\n","  plt.figure(figsize = (10,7))\n","  sn.set(font_scale=1.4)#for label size\n","  sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16},fmt='g')# font size\n","  plt.savefig(aa+'cm.eps')\n","  \n","  return model"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"7JBNy-hSmYBd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":340},"executionInfo":{"status":"error","timestamp":1600808150462,"user_tz":-360,"elapsed":4821,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"1411e062-2a89-4de6-e745-e56662a520e5"},"source":["model = CNN_LSTM(aa/'ulm_train.csv',aa/'ulm_test.csv','aa',6)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Loading data...\n","Creating vocab...\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-643a0e2284fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'ulm_train.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'ulm_test.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'aa'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-11-986e5953d160>\u001b[0m in \u001b[0;36mCNN_LSTM\u001b[0;34m(trainfile, testfile, aa, cat_output)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Creating vocab...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAX_NB_WORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_vocab_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Form Word Embedding Matrix...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-04712c3155f2>\u001b[0m in \u001b[0;36mcreate_vocab_set\u001b[0;34m(data, MAX_SEQUENCE_LENGTH)\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# split the 's'(data) and update on 'vocab',so vocab has all word in dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# open 'rb' as a input file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# python pickle is used to serialize and deserialize a python object structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'we' is not defined"]}]},{"cell_type":"code","metadata":{"id":"tJ6FEWl-mdJr","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}