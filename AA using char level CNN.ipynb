{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AA using char level CNN.ipynb","provenance":[],"authorship_tag":"ABX9TyMGfoINrDe0ZPXCMOwCm20+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"oAhCF9xwjpZh"},"source":["import string\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn import model_selection, preprocessing\n","from sklearn.model_selection import train_test_split\n","import io\n","import matplotlib.pyplot as plt\n"," \n","from __future__ import print_function\n","from __future__ import division\n","import json\n","import numpy as np\n"," \n","from keras.models import Model\n","from keras.optimizers import SGD, Adam\n","from keras.layers import Input, Dense, Dropout, Flatten, Lambda, Embedding,LSTM\n","from keras.layers.convolutional import Convolution1D, MaxPooling1D\n","from keras.initializers import RandomNormal\n"," \n","from sklearn.metrics import classification_report,confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","import matplotlib.pyplot as plt\n"," \n","import tensorflow as tf\n","from sklearn.metrics import f1_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_vuZFmMjy1q","executionInfo":{"status":"ok","timestamp":1600790764947,"user_tz":-360,"elapsed":73073,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"0b2e5314-4a05-4cf0-a2bc-18a43214ef6b","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HgBkP-GZkewA"},"source":["import os\n","from pathlib import Path\n","\n","path = Path(os.getcwd())\n","path = path/'gdrive'/'My Drive'\n","#news=path/'Thesis Data'/'full doc csv'\n","aa=path/'Thesis Data'/'Our dataset'\n","#aa2=path/'Thesis Data'/'AA dataset'\n","#char=path/'Thesis Data'/'char-ulm'\n","#wiki_data = path/'Thesis Data'/'wiki_data'/'Wiki'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZgMI73phkLIq"},"source":["def load_ag_data(trainfile,testfile): # recieve trainfile and testfile as a parameter\n","  \n","  onehotencoder = preprocessing.OneHotEncoder(categories='auto') # define one hot encoder\n","\n","  ## TRAIN\n","  df = pd.read_csv(trainfile) # read the train csv file\n","  df = df.dropna() # this method allows the user to analyze and drop rows/columns with null values in diff ways\n","  df = df.sample(frac=1).reset_index(drop=True)#shuffle\n","  x_train = np.array(df['text']).reshape(-1,1) # reshape (-1,1) means row is as you have same row in text and column is 1.Now the text is 2D.\n","  # 'fit' computes the mean and std dev to be used for later scaling(just computation).'transform' uses a previously computed mean and std dev to autoscale the data(subtract mean from all values and divide it by std dev)  \n","  y_train = onehotencoder.fit_transform((df['label'].values).reshape(-1,1)).toarray() # using 'label' with one hot encoding,reshape and toarray and predict the value which one store in y_train\n","\n","  ## TEST\n","  df = pd.read_csv(testfile)\n","  df = df.dropna()\n","  df = df.sample(frac=1).reset_index(drop=True)#shuffle\n","  x_test = np.array(df['text']).reshape(-1,1)\n","  y_test = onehotencoder.transform((df['label'].values).reshape(-1,1)).toarray() # toarray return a dense ndarray representation of this matrix\n","\n","  return (x_train, y_train), (x_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EP65Ri-_kLd4"},"source":["def create_vocab_set():\n","    alphabet=\" !\\\"#$%&'()*+,-./0123456789:;=<>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrstuvwxyz{|}~¥§©±ঃঅআইঈউঊঋএঐওঔকখগঘঙচছজঝঞটঠডঢণতথদধনপফবভমযরলশষসহ়ািীুূৃেৈোৌ্ৎৗড়ঢ়য়০১২৩৪৫৬৭৮৯৷‘’‚“”‪™−√∝∞\"    \n","    vocab_size = len(alphabet)\n","    print(vocab_size)\n","    vocab = {}\n","    reverse_vocab = {}\n","    for ix, t in enumerate(alphabet):\n","        vocab[t] = ix # each alphabet stores in 'vocab' dictionary,means index,vocabulary(key-value) pair\n","        reverse_vocab[ix] = t # each index number stores in 'vocab' as a index,means vocab,index(key-value) pair \n","    return vocab, reverse_vocab, vocab_size, alphabet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5L_TwyXTk6Rx"},"source":["def encode_data(x, maxlen, vocab): # x is train data or test data,maxlen is 3000,vocab is dictionary where key-value pair is indices and alphabet,take from function's parameter\n","    # Iterate over the loaded data and create a matrix of size (len(x), maxlen)\n","    # Each character is encoded into a one-hot array\n","    # Chars not in the vocab are encoded as -1, into an all zero vector.\n","    input_data = np.zeros((len(x), maxlen), dtype=np.int) # create a matrix with zero,row size is len(x) and column size is maxlen\n","    for dix, sent in enumerate(x): # take the index and text of x(train data/test data)\n","        counter = 0\n","        for c in sent[0]: # loop over throuh each word in a sentence\n","            if counter >= maxlen: # if the counter is greater then maxlen,then pass it,nothing to do\n","                pass\n","            else:                  # if the counter is less then maxlen then get the index and update it\n","                ix = vocab.get(c, -1)  # get index from vocab dictionary, if not in vocab, return -1\n","                input_data[dix, counter] = ix # take the 'ix'(index) on 'dix' and 'counter' of 'input_data'\n","                counter += 1 # increase the counter\n","    return input_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kjQLtm2AlntU"},"source":["def create_model(filter_kernels, dense_outputs, maxlen, vocab_size, nb_filter, cat_output, w): # w describe 'weight'\n","    \n","    initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)\n","\n","    # Define what the input shape looks like\n","    inputs = Input(shape=(maxlen,), dtype='int64')\n","\n","    # print(w)\n","    embedded = Embedding(input_dim=vocab_size, output_dim=vocab_size , weights=[w])(inputs) # multiply 'inputs' with embedding,\n","    # where 'input_dim' is vocab_size and 'output_dim' is also vocab_size\n","\n","    # All the convolutional layers...,'nb_filter[0]' is 64,'nb_filter[1]' is 128,'nb_filter[2]' is 256,'nb_filter[3]' is 256\n","    # and kernel takes 7,then 3,then 1,then 1 (arbitrary)\n","    conv = Convolution1D(filters=nb_filter[0], kernel_size=filter_kernels[0], kernel_initializer=initializer,\n","                         padding='valid', activation='relu',\n","                         input_shape=(maxlen, vocab_size))(embedded) # mult by 'embedded',i/p shape is (maxlen,vocab_size)\n","    conv = MaxPooling1D(pool_size=3)(conv) # here using (3*3) max pool/avg pool\n","\n","    conv1 = Convolution1D(filters=nb_filter[1], kernel_size=filter_kernels[1], kernel_initializer=initializer,\n","                          padding='valid', activation='relu')(conv) # we don't create extra zero padding here\n","    conv1 = MaxPooling1D(pool_size=3)(conv1)\n","\n","    conv2 = Convolution1D(filters=nb_filter[2], kernel_size=filter_kernels[2], kernel_initializer=initializer,\n","                          padding='valid', activation='relu')(conv1)\n","    conv2 = MaxPooling1D(pool_size=3)(conv2)\n","\n","    conv3 = Convolution1D(filters=nb_filter[3], kernel_size=filter_kernels[3], kernel_initializer=initializer,\n","                          padding='valid', activation='relu')(conv2)\n","    conv3 = MaxPooling1D(pool_size=3)(conv3)\n","    conv3 = Flatten()(conv3)\n","\n","    # Two dense layers with dropout of .5\n","    z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(conv3))\n","\n","    # Output dense layer with softmax activation\n","    pred = Dense(cat_output, activation='softmax', name='output')(z) # predicted the o/p using softmax activation\n","\n","    model = Model(inputs=inputs, outputs=pred) # create a instance of Model where 'inputs' is inputs and outputs is 'pred'\n","\n","    sgd = SGD(lr=0.01, momentum=0.9) # here create a instance of SGD opt. where learning rate is=0.01 and momentum is=0.9\n","    adam = Adam(lr=0.001, decay=0.0001) # here create a instance of Adam opt\n","    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy']) # using adam optimizer where 'metrics'\n","    # is acuracy\n","\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1PUcMVGqlyE8"},"source":["def CharCNN(trainfile,testfile,aa,cat_output,pre=False,w=0):\n","  np.random.seed(123)  # for reproducibility\n","  # Maximum length. Longer gets chopped. Shorter gets padded.\n","  maxlen = 3000\n","  # Model params\n","  # Filters for conv layers\n","  nb_filter = [64,128,256,256]\n","  # Number of units in the dense layer\n","  dense_outputs = 512\n","  # Conv layer kernel size\n","  filter_kernels = [7, 3,1,1]\n","  # Compile/fit params\n","  batch_size = 128 # take batch_size is 128 and no of epochs is 15\n","  nb_epoch = 15\n","\n","  print('Loading data...')\n","  (xt, yt), (x_test, y_test) = load_ag_data(trainfile,testfile) # call the 'load_ag_data' function and pass these params\n","\n","  print('Creating vocab...')\n","  vocab, reverse_vocab, vocab_size, alphabet = create_vocab_set() # call this function and take these values\n","\n","  if not pre:\n","    w=np.eye(vocab_size) # if not 'pre' then create np eye matrix(diagonal matrix of vocab_size) which describe weight\n","  print('Build model...')\n","  model = create_model(filter_kernels, dense_outputs, maxlen, vocab_size,nb_filter, cat_output,w) # creare a CNN model  \n","  \n","  print('Encode data...')\n","  xt = encode_data(xt, maxlen, vocab) # pass the parms where 'xt' means train data,maxlen is 3000,vocab is alphabet with index\n","  x_test = encode_data(x_test, maxlen, vocab)\n","  print(xt[0])\n","\n","  print('Chars vocab: {}'.format(alphabet))\n","  print('Chars vocab size: {}'.format(vocab_size))\n","  print('X_train.shape: {}'.format(xt.shape))\n","  model.summary()\n","  print('Fit model...')\n","  # fit the model with mean and std dev,train data is 80% and test data is 20% with shuffle  \n","  hist = model.fit(xt, yt, validation_split=0.2 , batch_size=batch_size, epochs=nb_epoch, shuffle=True)\n","\n","  history = pd.DataFrame(hist.history)\n","  plt.figure(figsize=(12,12));\n","  plt.plot(history[\"loss\"],label='loss');\n","  plt.plot(history[\"val_loss\"], label='val-loss');\n","  plt.title(\"Loss with trained word vectors\");\n","  plt.legend();\n","  # plt.show();\n","  plt.savefig(aa+'loss.eps')\n","  \n","  Y_pred = model.predict(x_test) # predict the o/p using test datset\n","  y_pred = np.argmax(Y_pred, axis=1) # this function returns of the maximum values are returned along with the specified axis\n","  y_test = np.argmax(y_test,axis=1)\n","  # print(y_pred)\n","  # print(y_test)\n","  acc = (y_test==y_pred).sum()/len(y_pred) # if the predict is same as test then return 1,sum all and divide by predicted value\n","  f1 = f1_score(y_test, y_pred, average='macro') # find f1 score\n","  print(\"Accuracy=\",acc,\" F1=\",f1)\n","  cm=confusion_matrix(y_test, y_pred) # draw the confusion matrix using originl label and predicted value\n","  print(cm)\n","\n","  df_cm = pd.DataFrame(cm, range(cat_output),range(cat_output))\n","  plt.figure(figsize = (10,7))\n","  sn.set(font_scale=1.4)#for label size\n","  sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16},fmt='g')# font size\n","  plt.savefig(aa+'cm.eps')\n","  \n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5DnRCPlQlzfK","executionInfo":{"status":"error","timestamp":1600794705003,"user_tz":-360,"elapsed":1199,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"db3a6a70-5551-4fd4-e007-45cf7a9d7cd0","colab":{"base_uri":"https://localhost:8080/","height":231}},"source":["w = model.layers[1].get_weights()[0] # dense layer is a subclass of layer and it weights,which is a type of python list has two elements weight of the layer stored at layer.get_weights()[0] and bias is stored at layer.get_weights()[1]\n","w = np.array([np.array(xi) for xi in w]) # for each elment of w applying np array,then again apply np array on top of it\n","np.savetxt('embeds_alphanum.txt', w) # save 'w' on this text file\n","print(w)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-0a12ff24b410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# dense layer is a subclass of layer and it weights,which is a type of python list has two elements weight of the layer stored at layer.get_weights()[0] and bias is stored at layer.get_weights()[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for each elment of w applying np array,then again apply np array on top of it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'embeds_alphanum.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# save 'w' on this text file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"code","metadata":{"id":"vMmUSjpAonjm"},"source":["# model = CharCNN(news/'full_doc_train.csv',news/'full_doc_test.csv','news',12)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-8f4OBelmFCI","executionInfo":{"status":"error","timestamp":1600796736207,"user_tz":-360,"elapsed":8716,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"3a0f7c33-9269-4de8-ed49-93e407624b9d","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model = CharCNN(aa/'ulm_train.csv',aa/'ulm_test.csv','aa',6)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading data...\n","Creating vocab...\n","181\n","Build model...\n","Encode data...\n","[129 144 117 ... 144 127 153]\n","Chars vocab:  !\"#$%&'()*+,-./0123456789:;=<>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrstuvwxyz{|}~¥§©±ঃঅআইঈউঊঋএঐওঔকখগঘঙচছজঝঞটঠডঢণতথদধনপফবভমযরলশষসহ়ািীুূৃেৈোৌ্ৎৗড়ঢ়য়০১২৩৪৫৬৭৮৯৷‘’‚“”‪™−√∝∞\n","Chars vocab size: 181\n","X_train.shape: (1800, 3000)\n","Model: \"functional_13\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_7 (InputLayer)         [(None, 3000)]            0         \n","_________________________________________________________________\n","embedding_6 (Embedding)      (None, 3000, 181)         32761     \n","_________________________________________________________________\n","conv1d_24 (Conv1D)           (None, 2994, 64)          81152     \n","_________________________________________________________________\n","max_pooling1d_24 (MaxPooling (None, 998, 64)           0         \n","_________________________________________________________________\n","conv1d_25 (Conv1D)           (None, 996, 128)          24704     \n","_________________________________________________________________\n","max_pooling1d_25 (MaxPooling (None, 332, 128)          0         \n","_________________________________________________________________\n","conv1d_26 (Conv1D)           (None, 332, 256)          33024     \n","_________________________________________________________________\n","max_pooling1d_26 (MaxPooling (None, 110, 256)          0         \n","_________________________________________________________________\n","conv1d_27 (Conv1D)           (None, 110, 256)          65792     \n","_________________________________________________________________\n","max_pooling1d_27 (MaxPooling (None, 36, 256)           0         \n","_________________________________________________________________\n","flatten_6 (Flatten)          (None, 9216)              0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 512)               4719104   \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 512)               0         \n","_________________________________________________________________\n","output (Dense)               (None, 6)                 3078      \n","=================================================================\n","Total params: 4,959,615\n","Trainable params: 4,959,615\n","Non-trainable params: 0\n","_________________________________________________________________\n","Fit model...\n","Epoch 1/15\n"],"name":"stdout"},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-36dd6dae5931>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'ulm_train.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'ulm_test.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'aa'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-17-3140db66b6fd>\u001b[0m in \u001b[0;36mCharCNN\u001b[0;34m(trainfile, testfile, aa, cat_output, pre, w)\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fit model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;31m# fit the model with mean and std dev,train data is 80% and test data is 20% with shuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m:  indices[96,154] = -1 is not in [0, 181)\n\t [[node functional_13/embedding_6/embedding_lookup (defined at <ipython-input-17-3140db66b6fd>:38) ]] [Op:__inference_train_function_5668]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node functional_13/embedding_6/embedding_lookup:\n functional_13/embedding_6/embedding_lookup/5267 (defined at /usr/lib/python3.6/contextlib.py:81)\n\nFunction call stack:\ntrain_function\n"]}]},{"cell_type":"code","metadata":{"id":"OKDM4ZJdzphb"},"source":[""],"execution_count":null,"outputs":[]}]}